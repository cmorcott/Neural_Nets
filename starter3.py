# -*- coding: utf-8 -*-
"""starter3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gf3Ix-l6ntoTN1KIC5eHITB49rHVFiQH
"""

import sys
import random
import math
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader
import datetime
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, confusion_matrix


device = "cuda" if torch.cuda.is_available() else "cpu"



def read_mnist(file_name):

    data_set = []
    with open(file_name,'rt') as f:
        for line in f:
            line = line.replace('\n','')
            tokens = line.split(',')
            label = tokens[0]
            attribs = []
            for i in range(784):
                attribs.append(tokens[i+1])
            data_set.append([label,attribs])
    return(data_set)

def read_insurability(file_name):

    count = 0
    data = []
    with open(file_name,'rt') as f:
        for line in f:
            if count > 0:
                line = line.replace('\n','')
                tokens = line.split(',')
                if len(line) > 10:
                    x1 = float(tokens[0])
                    x2 = float(tokens[1])
                    x3 = float(tokens[2])
                    if tokens[3] == 'Good':
                        cls = 0
                    elif tokens[3] == 'Neutral':
                        cls = 1
                    else:
                        cls = 2
                    data.append([[cls],[x1,x2,x3]])
            count = count + 1
    return(data)

##OUR CODE BELOW##
class CustomInsurDataSet(torch.utils.data.Dataset):
    def __init__(self, file, scaler):
        self.df = read_insurability(file)
        self.sc = scaler

        # Extract features and labels from self.df
        features = [d[1] for d in self.df]
        labels = [d[0] for d in self.df]

        # Scale the features
        self.scaled_features = self.sc.fit_transform(features)

        # Convert labels to the correct format
        self.labels = [label[0] for label in labels]

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        scaled_features = self.scaled_features[idx]
        label = self.labels[idx]

        # Convert to tensors
        data_tensor = torch.tensor(scaled_features, dtype=torch.float32)
        label_tensor = torch.tensor(label, dtype=torch.long)
        # labels might just be an integer based on the class it outputs...
        return data_tensor, label_tensor

class CustomMNISTDataSet(torch.utils.data.Dataset):
    def __init__(self, file, scaler):
        self.df = read_mnist(file)
        self.sc = scaler

        # Extract features and labels from self.df
        features = ([list(map(float, downsample_image(d[1]))) for d in self.df])
        labels = [int(d[0]) for d in self.df]

        # Scale the features
        self.scaled_features = self.sc.fit_transform(features)

        # Convert labels to the correct format
        self.labels = labels

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        scaled_features = self.scaled_features[idx]
        label = self.labels[idx]

        # Convert to tensors
        data_tensor = torch.tensor(scaled_features, dtype=torch.float32)
        label_tensor = torch.tensor(label, dtype=torch.long)
        return data_tensor, label_tensor

def train(dataloader, model, loss_func, optimizer, lamb):
  model.train()
  train_loss = []

  now = datetime.datetime.now()
  for batch, (X, y) in enumerate(dataloader):
    # ignore the first time we see this
    # second time why is gpu better than cpu for this?
    X, y = X.to(device), y.to(device)

    # make some predictions and get the error
    pred = model(X)

    R1 = model.linear1.weight
    R1 = torch.mul(R1,R1)
    R1 = torch.sum(R1)
    R1 = torch.sqrt(R1)

    R2 = model.linear2.weight
    R2 = torch.mul(R2,R2)
    R2 = torch.sum(R2)
    R2 = torch.sqrt(R2)

    R3 = model.linear_out.weight
    R3 = torch.mul(R3,R3)
    R3 = torch.sum(R3)
    R3 = torch.sqrt(R3)

    R = R1 + R2 + R3

    loss = loss_func(pred, y) + lamb * R

    # where the magic happens
    # backpropogation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if batch % 10 == 0:
      loss, current = loss.item(), batch * len(X)
      iters = 10 * len(X)
      then = datetime.datetime.now()
      iters /= (then - now).total_seconds()
      print(f"loss: {loss:>6f} |w| {R1:>6f} [{current:>5d}/{17000}] ({iters:.1f} its/sec)")
      now = then
      train_loss.append(loss)
  return train_loss

#class for the simple ff nn
class SimpleFeedForward(nn.Module):
  def __init__(self):
    super(SimpleFeedForward, self).__init__()
    self.linear1 = nn.Linear(3, 32)
    self.relu1 = nn.LeakyReLU()
    self.linear2 = nn.Linear(32, 16)
    self.relu2 = nn.LeakyReLU()
    self.linear_out = nn.Linear(16, 3)

  def forward(self, x):
    x = self.linear1(x)
    x = self.relu1(x)
    x = self.linear2(x)
    x = self.relu2(x)
    x = self.linear_out(x)
    return softmax(x)

#class for the deep ff nn
class DeepFeedForward(nn.Module):
  def __init__(self):
    super(DeepFeedForward, self).__init__()
    self.linear1 = nn.Linear(196, 16)
    self.relu1 = nn.LeakyReLU()
    self.linear2 = nn.Linear(16, 32)
    self.relu2 = nn.LeakyReLU()
    self.linear3 = nn.Linear(32, 8)
    self.relu3 = nn.LeakyReLU()
    self.linear4 = nn.Linear(8, 32)
    self.relu4 = nn.LeakyReLU()
    self.linear5 = nn.Linear(32, 16)
    self.relu5 = nn.LeakyReLU()
    self.linear_out = nn.Linear(16, 10)

  def forward(self, x):
    x = self.linear1(x)
    x = self.relu1(x)
    x = self.linear2(x)
    x = self.relu2(x)
    x = self.linear3(x)
    x = self.relu3(x)
    x = self.linear4(x)
    x = self.relu4(x)
    x = self.linear5(x)
    x = self.relu5(x)
    x = self.linear_out(x)
    return x

def test(dataloader, model, loss_func):
  size = len(dataloader)
  num_batches = 0
  model.eval()
  test_loss = 0

  with torch.no_grad():
    for X, y in dataloader:
      X, y = X.to(device), y.to(device)
      pred = model(X)
      test_loss += loss_func(pred, y).item()
      num_batches = num_batches + 1
  test_loss /= num_batches
  print(f"Avg Loss: {test_loss:>8f}\n")
  return test_loss

def test_accuracy(dataloader, model, mnist=False):
    model.eval()
    correct = 0
    total = 0
    all_predicted = []
    all_targets = []

    with torch.no_grad():
        for data, target in dataloader:
            data, target = data.to(device), target.to(device)
            probabilities = model(data)
            _, predicted = torch.max(probabilities, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

            # Store predictions and actual targets
            all_predicted.extend(predicted.view(-1).cpu().numpy())
            all_targets.extend(target.view(-1).cpu().numpy())

    accuracy = 100 * correct / total
    print(f'Accuracy of the network on the test images: {accuracy} %')

    # Compute F1 score and Confusion Matrix
    num_classes = 10 if mnist else 3
    f1 = f1_score(all_targets, all_predicted, average='weighted', labels=np.arange(num_classes))
    conf_matrix = confusion_matrix(all_targets, all_predicted, labels=np.arange(num_classes))

    print(f'F1 Score: {f1}')
    print('Confusion Matrix:\n', conf_matrix)

    return accuracy, f1, conf_matrix

def plot_loss(train_loss, valid_loss):
    average_train_loss = [sum(epoch_loss) / len(epoch_loss) for epoch_loss in train_loss]

    epochs = range(1, len(average_train_loss) + 1)

    plt.figure(figsize=(10, 6))
    plt.plot(epochs, average_train_loss, 'b-', label='Average Training Loss')
    plt.plot(epochs, valid_loss, 'r-', label='Validation Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

def softmax(x):
    # Subtract max for numerical stability
    x_exp = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])
    return x_exp / torch.sum(x_exp, dim=1, keepdim=True)

def downsample_image(image, n=2):
    downsampled_image = []

    for row in range(0, 28, n):
        for col in range(0, 28, n):
            avg_pixel = 0
            for i in range(n):
                for j in range(n):
                    avg_pixel += float(image[(row+i)*28 + col+j])
            avg_pixel //= (n*n)
            downsampled_image.append(avg_pixel)

    return downsampled_image

def classify_insurability():
    # insert code to train simple FFNN and produce evaluation metrics
    #same arch as slides 12-8, use SGD optimizer(), implement softmax() ourselves
    # train one obsv at a time, exp with diff hypparams
    #output learning curves, final test res, why this is a bad idea, why hypparm and impact

    sc = MinMaxScaler()
    train_data = CustomInsurDataSet('three_train.csv', sc)
    validation_data = CustomInsurDataSet('three_valid.csv', sc)
    test_data = CustomInsurDataSet('three_test.csv', sc)
    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
    valid_loader = DataLoader(validation_data, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)

    ff = SimpleFeedForward().to(device)
    loss_func = nn.CrossEntropyLoss()
    #instead of using this, use a stoch grad desc (SGD), maybe use a fixed learning rate
    optimizer = torch.optim.Adam(ff.parameters(), lr=1e-3)
    #determined this emperically
    epochs = 100
    train_loss = []
    valid_loss = []
    print("Now training Simple Feed Forward Neural Net for Insurability")
    for t in range(epochs):
        print(f"Epoch {t+1}\n------------------------------- \n")
        losses = train(train_loader, ff, loss_func, optimizer, 0.01)
        train_loss.append(losses)
        valid_loss.append(test(valid_loader, ff, loss_func))

    ideal_epoch = valid_loss.index(min(valid_loss))
    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)
    accuracy = test_accuracy(test_loader, ff)
    print("Ideal Epoch Is: ", ideal_epoch)
    plot_loss(train_loss, valid_loss)



    # Could add a condition that interrupts training when the loss doesn't change much
    print('Done!')

def classify_mnist():
    sc = MinMaxScaler()
    train_data = CustomMNISTDataSet('mnist_train.csv', sc)
    validation_data = CustomMNISTDataSet('mnist_valid.csv', sc)
    test_data = CustomMNISTDataSet('mnist_test.csv', sc)
    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
    valid_loader = DataLoader(validation_data, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)

    ff = DeepFeedForward().to(device)
    loss_func = nn.CrossEntropyLoss()
    #instead of using this, use a stoch grad desc (SGD), maybe use a fixed learning rate
    optimizer = torch.optim.Adam(ff.parameters(), lr=1e-3)
    #determined this emperically
    epochs = 70
    train_loss = []
    valid_loss = []
    print("Now training Deep Feed Forward Neural Net for MNIST")
    for t in range(epochs):
        print(f"Epoch {t+1}\n------------------------------- \n")
        losses = train(train_loader, ff, loss_func, optimizer, 0.01)
        train_loss.append(losses)
        valid_loss.append(test(valid_loader, ff, loss_func))
    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)
    accuracy = test_accuracy(test_loader, ff, True)
    ideal_epoch = valid_loss.index(min(valid_loss))
    print("Ideal Epoch Is: ", ideal_epoch)
    plot_loss(train_loss, valid_loss)



    # Could add a condition that interrupts training when the loss doesn't change much
    print('Done!')

def classify_mnist_reg():
    sc = MinMaxScaler()
    train_data = CustomMNISTDataSet('mnist_train.csv', sc)
    validation_data = CustomMNISTDataSet('mnist_valid.csv', sc)
    test_data = CustomMNISTDataSet('mnist_test.csv', sc)
    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
    valid_loader = DataLoader(validation_data, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)

    ff = DeepFeedForward().to(device)
    loss_func = nn.CrossEntropyLoss()
    #instead of using this, use a stoch grad desc (SGD), maybe use a fixed learning rate
    optimizer = torch.optim.Adam(ff.parameters(), lr=1e-3, weight_decay=1e-5)
    #determined this emperically
    epochs = 100
    train_loss = []
    valid_loss = []
    print("Now training Deep Feed Forward Neural Net for MNIST")
    for t in range(epochs):
        print(f"Epoch {t+1}\n------------------------------- \n")
        losses = train(train_loader, ff, loss_func, optimizer, 0.01)
        train_loss.append(losses)
        valid_loss.append(test(valid_loader, ff, loss_func))
    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)
    accuracy = test_accuracy(test_loader, ff, True)
    ideal_epoch = valid_loss.index(min(valid_loss))
    print("Ideal Epoch Is: ", ideal_epoch)
    plot_loss(train_loss, valid_loss)



    # Could add a condition that interrupts training when the loss doesn't change much
    print('Done!')

def main():
    classify_insurability()
    classify_mnist()
    classify_mnist_reg()

if __name__ == "__main__":
    main()